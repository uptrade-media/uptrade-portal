/**
 * Setup Command - AI-powered comprehensive site SEO setup
 *
 * This command scans your entire codebase and uses Signal AI to:
 * - Extract existing FAQs from any pattern (imports, arrays, components)
 * - Generate or extract metadata for every page
 * - Generate appropriate schema markup (Organization, Service, LocalBusiness, etc.)
 * - Suggest internal links between pages
 * - Create database records in Portal
 * - Generate code snippets for managed components
 *
 * IMPORTANT SCHEMA ARCHITECTURE:
 * - FAQPage schema â†’ Generated by <ManagedFAQ /> component (not here)
 * - Article schema â†’ Generated during blog post creation (not here)
 * - Other schema â†’ Generated here and stored in seo_pages.managed_schema
 * 
 * This ensures FAQ/blog content changes automatically update their schemas
 * without requiring page schema regeneration.
 *
 * Usage:
 *   npx uptrade-setup setup                    # Full setup
 *   npx uptrade-setup setup --dry-run          # Preview without changes
 *   npx uptrade-setup setup --faqs-only        # Only extract/setup FAQs
 *   npx uptrade-setup setup --schema-only      # Only generate schema
 */

import chalk from 'chalk'
import ora from 'ora'
import path from 'path'
import fs from 'fs/promises'
import { existsSync, readFileSync } from 'fs'
import { scanCodebase } from '../scanner'

// Function to get API URL (reads env vars at runtime after dotenv has loaded)
function getApiUrl(): string {
  return process.env.UPTRADE_API_URL || process.env.NEXT_PUBLIC_UPTRADE_API_URL || 'https://api.uptrademedia.com'
}

interface Config {
  projectId: string
  apiKey: string
  siteUrl?: string
  businessInfo?: {
    name?: string
    type?: string
    description?: string
    phone?: string
    email?: string
    address?: {
      street?: string
      city?: string
      state?: string
      zip?: string
    }
    services?: string[]
  }
}

interface SetupOptions {
  dryRun?: boolean
  faqsOnly?: boolean
  schemaOnly?: boolean
  metadataOnly?: boolean
  verbose?: boolean
}

interface PageFile {
  filePath: string
  pagePath: string
  content: string
  importedFiles: Record<string, string>
}

interface PageAnalysis {
  path: string
  file_path: string
  page_type: string
  faqs?: {
    path: string
    title?: string
    items: Array<{ question: string; answer: string }>
    source_file: string
  }
  metadata?: {
    path: string
    title: string
    description: string
    keywords?: string[]
    source: 'extracted' | 'generated'
  }
  schemas?: Array<{
    path: string
    schema_type: string
    schema_json: Record<string, unknown>
    source: 'extracted' | 'generated'
  }>
  insertions?: Array<{
    component: string
    props: Record<string, unknown>
    code_snippet: string
    insertion_hint: string
  }>
}

interface SiteSetupResult {
  pages: PageAnalysis[]
  global_schema?: {
    path: string
    schema_type: string
    schema_json: Record<string, unknown>
    source: 'extracted' | 'generated'
  }
  summary: {
    total_pages: number
    faqs_extracted: number
    faqs_items_total: number
    metadata_generated: number
    schemas_generated: number
  }
}

async function loadConfig(): Promise<Config | null> {
  const configPath = path.join('.uptrade', 'config.json')
  if (existsSync(configPath)) {
    try {
      const config = JSON.parse(readFileSync(configPath, 'utf-8'))
      return {
        projectId: config.projectId || config.project_id,
        apiKey: config.apiKey || config.api_key,
        siteUrl: config.siteUrl || config.site_url,
        businessInfo: config.businessInfo || config.business_info,
      }
    } catch {
      return null
    }
  }

  const projectId = process.env.NEXT_PUBLIC_UPTRADE_PROJECT_ID || process.env.UPTRADE_PROJECT_ID
  const apiKey = process.env.UPTRADE_API_KEY
  const siteUrl = process.env.NEXT_PUBLIC_SITE_URL || process.env.SITE_URL

  if (projectId && apiKey) {
    return { projectId, apiKey, siteUrl }
  }
  return null
}

export async function setupCommand(options: SetupOptions) {
  console.log('')
  console.log(chalk.bold.blue('  ðŸš€ Signal-Enhanced SEO Setup'))
  console.log(chalk.gray('  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€'))
  console.log('')

  // Load config
  const config = await loadConfig()
  if (!config?.projectId || !config?.apiKey) {
    console.log(chalk.red('  âœ— Not configured. Run `npx uptrade-setup init` first.'))
    console.log(chalk.gray('    Set NEXT_PUBLIC_UPTRADE_PROJECT_ID and UPTRADE_API_KEY'))
    console.log('')
    return
  }

  // Determine capabilities based on options
  const capabilities: string[] = []
  if (options.faqsOnly) {
    capabilities.push('extract_faqs')
  } else if (options.schemaOnly) {
    capabilities.push('generate_schema', 'extract_existing_schema')
  } else if (options.metadataOnly) {
    capabilities.push('generate_metadata', 'extract_existing_metadata')
  } else {
    // Full setup
    capabilities.push(
      'extract_faqs',
      'generate_metadata',
      'extract_existing_metadata',
      'generate_schema',
      'extract_existing_schema',
      'generate_internal_links'
    )
  }

  console.log(chalk.white('  Capabilities: ') + chalk.cyan(capabilities.join(', ')))
  console.log('')

  // Step 1: Scan codebase for pages
  const scanSpinner = ora('Scanning codebase for pages...').start()
  let pageFiles: PageFile[] = []

  try {
    pageFiles = await scanForPageFiles(process.cwd())
    scanSpinner.succeed(`Found ${pageFiles.length} page(s) to analyze`)
  } catch (error) {
    scanSpinner.fail('Scan failed')
    console.log(chalk.red(`  ${(error as Error).message}`))
    return
  }

  if (pageFiles.length === 0) {
    console.log(chalk.yellow('  No pages found. Looking for app/ or pages/ directory.'))
    console.log('')
    return
  }

  // Step 1.5: Expand dynamic routes into individual pages
  const expandSpinner = ora('Expanding dynamic routes...').start()
  try {
    pageFiles = await expandDynamicRoutes(pageFiles, process.cwd())
    expandSpinner.succeed(`Expanded to ${pageFiles.length} page(s) (including dynamic routes)`)
  } catch (error) {
    expandSpinner.fail('Failed to expand dynamic routes')
    console.log(chalk.red(`  ${(error as Error).message}`))
    // Continue with original pages
  }

  // Show pages found
  if (options.verbose) {
    console.log('')
    console.log(chalk.gray('  Pages found:'))
    for (const page of pageFiles.slice(0, 15)) {
      const isDynamic = page.importedFiles['__dynamic_item__'] ? chalk.cyan(' [dynamic]') : ''
      console.log(chalk.gray(`    ${page.pagePath}`) + isDynamic)
    }
    if (pageFiles.length > 15) {
      console.log(chalk.gray(`    ... and ${pageFiles.length - 15} more`))
    }
    console.log('')
  }

  // Step 2: Gather context for each page
  const contextSpinner = ora('Gathering page context and imports...').start()

  try {
    for (const page of pageFiles) {
      await gatherImportedFiles(page, process.cwd())
    }
    contextSpinner.succeed('Gathered context for all pages')
  } catch (error) {
    contextSpinner.fail('Failed to gather context')
    console.log(chalk.red(`  ${(error as Error).message}`))
    return
  }

  // Dry run - show what would be sent
  if (options.dryRun) {
    console.log('')
    console.log(chalk.yellow('  [DRY RUN] Would analyze:'))
    console.log('')
    for (const page of pageFiles) {
      const isDynamic = page.importedFiles['__dynamic_item__'] ? chalk.cyan(' [dynamic]') : ''
      console.log(chalk.white(`    ${page.pagePath}`) + isDynamic)
      console.log(chalk.gray(`      File: ${page.filePath}`))
      const importCount = Object.keys(page.importedFiles).length
      if (importCount > 0) {
        console.log(chalk.gray(`      Imports resolved: ${importCount}`))
      }
    }
    console.log('')
    console.log(chalk.gray('  Run without --dry-run to execute.'))
    console.log('')
    return
  }

  // Step 3: Send to Signal AI for analysis in batches
  const BATCH_SIZE = 3  // Process 3 pages at a time to avoid timeout
  const batches: PageFile[][] = []
  for (let i = 0; i < pageFiles.length; i += BATCH_SIZE) {
    batches.push(pageFiles.slice(i, i + BATCH_SIZE))
  }

  console.log('')
  console.log(chalk.bold(`  ðŸ¤– Analyzing ${pageFiles.length} pages in ${batches.length} batches`))
  console.log('')

  const allResults: PageAnalysis[] = []
  let globalSchema: GeneratedSchema | undefined

  for (let batchIndex = 0; batchIndex < batches.length; batchIndex++) {
    const batch = batches[batchIndex]
    const batchSpinner = ora(`Batch ${batchIndex + 1}/${batches.length}: Analyzing ${batch.length} pages...`).start()

    try {
      const response = await fetch(`${getApiUrl()}/api/public/seo/site-setup`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'x-api-key': config.apiKey,
        },
        body: JSON.stringify({
          project_id: config.projectId,
          site_url: config.siteUrl || 'https://example.com',
          // Only send business_info on first batch (for org schema)
          business_info: batchIndex === 0 ? config.businessInfo : undefined,
          files: batch.map(p => ({
            file_path: p.filePath,
            page_path: p.pagePath,
            content: p.content,
            imported_files: p.importedFiles,
          })),
          capabilities,
        }),
      })

      if (!response.ok) {
        const errorText = await response.text()
        throw new Error(`API error: ${response.status} ${errorText}`)
      }

      const batchResult: SiteSetupResult = await response.json()
      allResults.push(...batchResult.pages)
      
      // Capture global schema from first batch
      if (batchIndex === 0 && batchResult.global_schema) {
        globalSchema = batchResult.global_schema
      }

      batchSpinner.succeed(`Batch ${batchIndex + 1}/${batches.length}: Analyzed ${batchResult.pages.length} pages`)

      // IMMEDIATELY upload FAQs, schemas while context is fresh
      console.log(chalk.gray(`  â”œâ”€ Uploading batch results while context is fresh...`))
      
      // Upload FAQs for this batch
      const batchFaqPages = batchResult.pages.filter(p => p.faqs?.items.length)
      if (batchFaqPages.length > 0) {
        await uploadFaqs(batchFaqPages, config)
        const faqItems = batchFaqPages.reduce((sum, p) => sum + (p.faqs?.items.length || 0), 0)
        console.log(chalk.gray(`  â”œâ”€ âœ“ Uploaded ${batchFaqPages.length} FAQ sections (${faqItems} items)`))
      }

      // Upload schemas for this batch
      const batchSchemaPages = batchResult.pages.filter(p => p.schemas?.length)
      if (batchSchemaPages.length > 0) {
        await uploadSchemas(batchSchemaPages, batchIndex === 0 ? globalSchema : undefined, config)
        console.log(chalk.gray(`  â”œâ”€ âœ“ Uploaded ${batchSchemaPages.length} schema markups`))
      }

      // Count metadata
      const batchMetadata = batchResult.pages.filter(p => p.metadata).length
      if (batchMetadata > 0) {
        console.log(chalk.gray(`  â””â”€ âœ“ Generated metadata for ${batchMetadata} pages`))
      }

      console.log('')

    } catch (error) {
      batchSpinner.fail(`Batch ${batchIndex + 1}/${batches.length} failed`)
      console.log(chalk.red(`    ${(error as Error).message}`))
      console.log('')
      // Continue with next batch instead of aborting entirely
    }
  }

  if (allResults.length === 0) {
    console.log(chalk.red('\n  No pages were analyzed successfully.'))
    return
  }

  // Summary
  console.log('')
  console.log(chalk.bold('  ðŸ“Š Setup Complete'))
  console.log(chalk.gray('  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€'))
  console.log('')
  console.log(chalk.green(`  âœ“ Analyzed: ${allResults.length} pages`))
  
  const totalFaqPages = allResults.filter(p => p.faqs?.items.length).length
  if (totalFaqPages > 0) {
    const totalFaqItems = allResults.reduce((sum, p) => sum + (p.faqs?.items.length || 0), 0)
    console.log(chalk.green(`  âœ“ FAQs: ${totalFaqPages} sections, ${totalFaqItems} items uploaded`))
  }
  
  const totalSchemaPages = allResults.filter(p => p.schemas?.length).length
  if (totalSchemaPages > 0) {
    console.log(chalk.green(`  âœ“ Schema: ${totalSchemaPages} markup(s) uploaded`))
  }
  
  const totalMetadataPages = allResults.filter(p => p.metadata).length
  if (totalMetadataPages > 0) {
    console.log(chalk.green(`  âœ“ Metadata: ${totalMetadataPages} pages generated`))
  }

  console.log('')

  // Step 5: Show component insertion instructions
  console.log(chalk.bold('  ðŸ“ Component Insertions'))
  console.log(chalk.gray('  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€'))
  console.log('')

  for (const page of allResults) {
    if (page.insertions && page.insertions.length > 0) {
      console.log(chalk.white(`  ${page.path}`))
      for (const insertion of page.insertions) {
        console.log(chalk.gray(`    â†’ ${insertion.component}`))
        console.log(chalk.green(`      ${insertion.code_snippet}`))
      }
      console.log('')
    }
  }

  console.log(chalk.bold.green('  âœ“ Setup complete!'))
  console.log('')
  console.log(chalk.gray('  Next steps:'))
  console.log(chalk.gray('    1. Add managed components to your pages (see insertions above)'))
  console.log(chalk.gray('    2. Run `npx uptrade-setup verify` to validate'))
  console.log(chalk.gray('    3. Deploy and check Portal SEO dashboard'))
  console.log('')
}

/**
 * Scan for page files in the codebase
 */
async function scanForPageFiles(rootDir: string): Promise<PageFile[]> {
  const pages: PageFile[] = []

  // Check for Next.js app directory
  const appDir = path.join(rootDir, 'app')
  const srcAppDir = path.join(rootDir, 'src', 'app')
  const pagesDir = path.join(rootDir, 'pages')
  const srcPagesDir = path.join(rootDir, 'src', 'pages')

  const dirs = [appDir, srcAppDir, pagesDir, srcPagesDir]

  for (const dir of dirs) {
    if (existsSync(dir)) {
      await scanDirectory(dir, dir, pages)
    }
  }

  return pages
}

/**
 * Recursively scan directory for page files
 */
async function scanDirectory(dir: string, baseDir: string, pages: PageFile[]): Promise<void> {
  const entries = await fs.readdir(dir, { withFileTypes: true })

  for (const entry of entries) {
    const fullPath = path.join(dir, entry.name)

    if (entry.isDirectory()) {
      // Skip node_modules, hidden dirs, etc.
      if (entry.name.startsWith('.') || entry.name === 'node_modules' || entry.name === 'api') {
        continue
      }
      await scanDirectory(fullPath, baseDir, pages)
    } else if (entry.isFile()) {
      // Look for page files
      const isPage = entry.name.match(/^page\.(tsx?|jsx?)$/)
      const isLegacyPage = baseDir.includes('/pages') && entry.name.match(/\.(tsx?|jsx?)$/) && !entry.name.startsWith('_')

      if (isPage || isLegacyPage) {
        const content = await fs.readFile(fullPath, 'utf-8')
        const relativePath = path.relative(baseDir, fullPath)

        // Calculate page path from file path
        let pagePath = '/' + path.dirname(relativePath)
          .replace(/\\/g, '/')
          .replace(/\/page$/, '')

        // Handle dynamic routes
        pagePath = pagePath
          .replace(/\[([^\]]+)\]/g, ':$1')
          .replace(/^\/\.$/, '/')

        if (pagePath === '/.') pagePath = '/'

        pages.push({
          filePath: relativePath,
          pagePath,
          content,
          importedFiles: {},
        })
      }
    }
  }
}

/**
 * Gather imported files for a page (FAQs, data files, etc.)
 */
async function gatherImportedFiles(page: PageFile, rootDir: string): Promise<void> {
  const importMatches = page.content.matchAll(/import\s+(?:\{[^}]+\}|\w+)\s+from\s+['"]([^'"]+)['"]/g)

  for (const match of importMatches) {
    const importPath = match[1]

    // Look for relevant imports (expanded list for dynamic routes)
    if (
      importPath.toLowerCase().includes('faq') ||
      importPath.includes('@/data') ||
      importPath.includes('./data') ||
      importPath.includes('../data') ||
      importPath.includes('/constants') ||
      importPath.includes('/content') ||
      importPath.includes('practice-areas') ||
      importPath.includes('subpages') ||
      importPath.includes('pages-data')
    ) {
      let resolvedPath = importPath

      // Resolve path aliases
      if (importPath.startsWith('@/')) {
        resolvedPath = path.join(rootDir, importPath.replace('@/', ''))
      } else if (importPath.startsWith('.')) {
        const pageDir = path.dirname(path.join(rootDir, 'app', page.filePath))
        resolvedPath = path.resolve(pageDir, importPath)
      }

      // Try common extensions
      const extensions = ['', '.js', '.ts', '.jsx', '.tsx', '.json']
      for (const ext of extensions) {
        const tryPath = resolvedPath + ext
        if (existsSync(tryPath)) {
          try {
            page.importedFiles[importPath] = await fs.readFile(tryPath, 'utf-8')
          } catch {
            // Skip if can't read
          }
          break
        }
      }
    }
  }
}

/**
 * Expand dynamic [slug] routes into individual page entries
 * Looks for data files that define the slugs (e.g., practice-areas.js)
 */
async function expandDynamicRoutes(pages: PageFile[], rootDir: string): Promise<PageFile[]> {
  const expandedPages: PageFile[] = []
  const dynamicRoutePattern = /\/:(\w+)$/

  for (const page of pages) {
    // Check if this is a dynamic route
    if (page.pagePath.includes(':')) {
      // This is a dynamic [slug] route
      const slugMatch = page.pagePath.match(dynamicRoutePattern)
      if (slugMatch) {
        const parentPath = page.pagePath.replace(dynamicRoutePattern, '')
        const slugParam = slugMatch[1]

        // Try to find dynamic route data
        const dynamicData = await findDynamicRouteData(page, parentPath, rootDir)

        if (dynamicData.length > 0) {
          // Expand into individual pages
          for (const item of dynamicData) {
            const expandedPath = parentPath + '/' + item.slug
            expandedPages.push({
              filePath: page.filePath,
              pagePath: expandedPath,
              content: page.content,
              importedFiles: {
                ...page.importedFiles,
                // Add the specific item data as JSON
                '__dynamic_item__': JSON.stringify(item.data, null, 2),
              },
            })
          }
          // Also keep the template for reference
          expandedPages.push(page)
        } else {
          // No dynamic data found, keep original
          expandedPages.push(page)
        }
      } else {
        expandedPages.push(page)
      }
    } else {
      // Static route, keep as-is
      expandedPages.push(page)
    }
  }

  return expandedPages
}

/**
 * Find data that populates a dynamic route
 */
async function findDynamicRouteData(
  page: PageFile,
  parentPath: string,
  rootDir: string
): Promise<Array<{ slug: string; data: Record<string, unknown> }>> {
  const results: Array<{ slug: string; data: Record<string, unknown> }> = []

  // Common data file locations
  const dataFilePaths = [
    path.join(rootDir, 'data', 'practice-areas.js'),
    path.join(rootDir, 'data', 'practice-areas.ts'),
    path.join(rootDir, 'data', 'personal-injury-pages.js'),
    path.join(rootDir, 'data', 'personal-injury-pages.ts'),
    path.join(rootDir, 'data', 'pages.js'),
    path.join(rootDir, 'data', 'pages.ts'),
    path.join(rootDir, 'data', 'content.js'),
    path.join(rootDir, 'data', 'content.ts'),
    path.join(rootDir, 'data', 'services.js'),
    path.join(rootDir, 'data', 'services.ts'),
    path.join(rootDir, 'data', 'subpages.js'),
    path.join(rootDir, 'data', 'subpages.ts'),
    path.join(rootDir, 'src', 'data', 'practice-areas.js'),
    path.join(rootDir, 'src', 'data', 'practice-areas.ts'),
  ]

  // Get the route segment (e.g., "adoption" from "/adoption/:slug")
  const routeSegment = parentPath.replace(/^\//, '').split('/')[0]
  // Also create a version with hyphens removed for matching (personal-injury -> personalinjury)
  const routeSegmentNormalized = routeSegment.replace(/-/g, '')

  for (const dataFile of dataFilePaths) {
    if (!existsSync(dataFile)) continue

    try {
      const content = await fs.readFile(dataFile, 'utf-8')

      // Look for exported objects that match the route segment
      // Pattern: export const adoptionSubpages = { ... }
      // Also match with hyphens converted (personal-injury -> personalInjury or personalinjury)
      const exportPattern = new RegExp(
        `export\\s+const\\s+(${routeSegment.replace(/-/g, '[\\-_]?')}\\w*|\\w*${routeSegmentNormalized}\\w*)\\s*=\\s*\\{`,
        'i'
      )
      const match = content.match(exportPattern)

      if (match) {
        // Found a matching export, parse the object keys as slugs
        const slugs = extractObjectKeys(content, match.index || 0)

        for (const slug of slugs) {
          // Extract the data for this slug
          const itemData = extractObjectItem(content, match.index || 0, slug)
          results.push({
            slug,
            data: itemData,
          })
        }

        if (results.length > 0) {
          // Store the data file content for AI analysis
          page.importedFiles[`__dynamic_data_${routeSegment}__`] = content
          break
        }
      }
    } catch {
      // Skip if can't read/parse
    }
  }

  return results
}

/**
 * Extract object keys from JS/TS object definition
 */
function extractObjectKeys(content: string, startIndex: number): string[] {
  const keys: string[] = []

  // Find the opening brace
  let braceStart = content.indexOf('{', startIndex)
  if (braceStart === -1) return keys

  // Simple regex to find top-level keys
  // Look for patterns like: keyName: { or 'keyName': {
  const keyPattern = /^\s*(['"]?)(\w+(?:-\w+)*)\1\s*:\s*\{/gm

  // Get content after the export and before the matching closing brace
  let braceCount = 1
  let i = braceStart + 1
  let objectContent = ''

  while (i < content.length && braceCount > 0) {
    const char = content[i]
    objectContent += char
    if (char === '{') braceCount++
    if (char === '}') braceCount--
    i++
  }

  // Find keys in the object content
  let match
  // Reset lastIndex for global regex
  keyPattern.lastIndex = 0

  // Only match at the first level of nesting
  let nestLevel = 0
  const lines = objectContent.split('\n')

  for (const line of lines) {
    // Track nesting level
    nestLevel += (line.match(/\{/g) || []).length
    nestLevel -= (line.match(/\}/g) || []).length

    // Only look for keys at first nesting level
    if (nestLevel <= 1) {
      const keyMatch = line.match(/^\s*(['"]?)(\w+(?:-\w+)*)\1\s*:\s*\{/)
      if (keyMatch) {
        keys.push(keyMatch[2])
      }
    }
  }

  return keys
}

/**
 * Extract a specific item's data from an object definition
 */
function extractObjectItem(
  content: string,
  startIndex: number,
  key: string
): Record<string, unknown> {
  // Find the key in the content
  const keyPattern = new RegExp(`(['"]?)${key}\\1\\s*:\\s*\\{`)
  const match = content.slice(startIndex).match(keyPattern)

  if (!match) return { slug: key }

  const keyStart = startIndex + (match.index || 0)
  const braceStart = content.indexOf('{', keyStart + match[0].length - 1)

  // Extract until matching closing brace
  let braceCount = 1
  let i = braceStart + 1
  let objectContent = '{'

  while (i < content.length && braceCount > 0) {
    const char = content[i]
    objectContent += char
    if (char === '{') braceCount++
    if (char === '}') braceCount--
    i++
  }

  // Try to extract key fields from the object
  const result: Record<string, unknown> = { slug: key }

  // Extract title
  const titleMatch = objectContent.match(/title\s*:\s*['"]([^'"]+)['"]/)
  if (titleMatch) result.title = titleMatch[1]

  // Extract subtitle
  const subtitleMatch = objectContent.match(/subtitle\s*:\s*['"]([^'"]+)['"]/)
  if (subtitleMatch) result.subtitle = subtitleMatch[1]

  // Extract description
  const descMatch = objectContent.match(/description\s*:\s*['"]([^'"]+)['"]/)
  if (descMatch) result.description = descMatch[1]

  // Store raw content for AI analysis
  result.__raw_content__ = objectContent

  return result
}

/**
 * Upload extracted FAQs to Portal
 */
async function uploadFaqs(pages: PageAnalysis[], config: Config): Promise<void> {
  for (const page of pages) {
    if (!page.faqs?.items?.length) continue

    try {
      const res = await fetch(`${getApiUrl()}/api/public/seo/register-faq`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'x-api-key': config.apiKey,
        },
        body: JSON.stringify({
          path: page.faqs.path,
          title: page.faqs.title || 'Frequently Asked Questions',
          items: page.faqs.items.map((item, index) => ({
            id: `faq-${index + 1}`,
            question: item.question,
            answer: item.answer,
            order: index,
            is_visible: true,
          })),
          include_schema: true,
          is_published: true,
        }),
      })

      if (res.ok) {
        console.log(chalk.green(`    âœ“ FAQ: ${page.faqs.path}`) + chalk.gray(` (${page.faqs.items.length} items)`))
      } else if (res.status === 409) {
        console.log(chalk.blue(`    â†» FAQ: ${page.faqs.path}`) + chalk.gray(' (already exists)'))
      } else {
        console.log(chalk.red(`    âœ— FAQ: ${page.faqs.path}: ${await res.text()}`))
      }
    } catch (error) {
      console.log(chalk.red(`    âœ— FAQ: ${page.faqs.path}: ${(error as Error).message}`))
    }
  }
}

/**
 * Upload generated schemas to Portal
 */
async function uploadSchemas(
  pages: PageAnalysis[],
  globalSchema: SiteSetupResult['global_schema'],
  config: Config
): Promise<void> {
  // Upload global org schema
  if (globalSchema) {
    try {
      const res = await fetch(`${getApiUrl()}/api/public/seo/register-schema`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'x-api-key': config.apiKey,
        },
        body: JSON.stringify({
          page_path: '/',
          schema_type: globalSchema.schema_type,
          schema_json: globalSchema.schema_json,
          is_implemented: true,
        }),
      })

      if (res.ok) {
        console.log(chalk.green(`    âœ“ Schema: / (${globalSchema.schema_type})`))
      } else if (res.status === 409) {
        console.log(chalk.blue(`    â†» Schema: / (${globalSchema.schema_type})`) + chalk.gray(' (already exists)'))
      }
    } catch (error) {
      console.log(chalk.red(`    âœ— Schema: / - ${(error as Error).message}`))
    }
  }

  // Upload page schemas
  for (const page of pages) {
    if (!page.schemas?.length) continue

    for (const schema of page.schemas) {
      try {
        const res = await fetch(`${getApiUrl()}/api/public/seo/register-schema`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'x-api-key': config.apiKey,
          },
          body: JSON.stringify({
            page_path: schema.path,
            schema_type: schema.schema_type,
            schema_json: schema.schema_json,
            is_implemented: true,
          }),
        })

        if (res.ok) {
          console.log(chalk.green(`    âœ“ Schema: ${schema.path} (${schema.schema_type})`))
        } else if (res.status === 409) {
          console.log(chalk.blue(`    â†» Schema: ${schema.path}`) + chalk.gray(' (already exists)'))
        }
      } catch (error) {
        console.log(chalk.red(`    âœ— Schema: ${schema.path} - ${(error as Error).message}`))
      }
    }
  }
}
